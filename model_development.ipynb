{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM, TimeDistributed, Conv1D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "\n",
    "\n",
    "from keras.utils import vis_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seya.layers.ntm import NeuralTuringMachine as NTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ConfigurableNetwork:\n",
    "    defaultfile = 'default.cfg'\n",
    "    def __init__(self, modelname):\n",
    "        self._modelname = modelname\n",
    "\n",
    "        # Set up the environment. load config file if it is there, if not, then create it form default\n",
    "        # Load the weights\n",
    "\n",
    "        # existential question: should this contain all of the handling for fitting and callbacks?\n",
    "        # yeah I think that makes sense. One stop shop for configs, weights, and logging.\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def modelname(self): return self._modelname\n",
    "\n",
    "class DeepMemNet:\n",
    "    \"\"\"\n",
    "    DeepMemNet for the Facebook bAbI context task.\n",
    "\n",
    "    Model notes:\n",
    "    Single context task:\n",
    "      Regular LSTM (accuracy/val_acc):\n",
    "        Run 1\n",
    "            47/50% @ 7 epochs\n",
    "            72/70% @ 49 epochs\n",
    "            86/80% @ 61 epochs\n",
    "            95/90% @ 88 epochs\n",
    "        Run 2\n",
    "\n",
    "\n",
    "\n",
    "      Bidirectional LSTM:\n",
    "        Run 1\n",
    "            50%/50% @ 6\n",
    "            81%/80% @ 34\n",
    "            87%/86% @ 48 (peak valacc)\n",
    "            90%/86% @ 60 epochs - minor overfitting\n",
    "\n",
    "        I think parameters were not configured right, these might be Single LSTM! need to rerun everything >_<\n",
    "        Run 2\n",
    "            73/71% @ 44\n",
    "            83/80% @ 53\n",
    "            94/90% @ 75\n",
    "        Run 3\n",
    "            71/70% @ 41\n",
    "            83/80% @ 50\n",
    "            94/90% @ 76\n",
    "\n",
    "      Bidirectional + extra forward LSTM:\n",
    "        55%/??% @ 80 (stalls)\n",
    "\n",
    "      TDDense + Bidirectional:\n",
    "        Run 1:\n",
    "            76/73% @ 37\n",
    "            80/80% @ 41\n",
    "            91/90% @ 54 - new record!!\n",
    "\n",
    "    Double Context task:\n",
    "      Regular:\n",
    "        50%/??% @ 35 epochs\n",
    "        67%/??% @ 80 epochs\n",
    "        70%/??% @ 100\n",
    "        80%/??% @ 192\n",
    "        84.7%/??% @ 260\n",
    "\n",
    "      Bidirectional:\n",
    "        50%/??% @ 26 epochs\n",
    "        70%/??% @ 48 epochs - improvement!\n",
    "        80%/??% @ 68 epochs - super improvement!\n",
    "        90%/??% @ 110 epochs - smokin'!\n",
    "        95%/??% @ 148 epochs - starting to level off\n",
    "        97%/??% @ 200 epochs - i think it's starting to overfit\n",
    "    \"\"\"\n",
    "    # todo: add performance logging\n",
    "    def __init__(self, vocab_size=22, story_maxlen=68, query_maxlen=4, n_lstm=32, bidirect=True, tdd=True):\n",
    "        \"\"\"\n",
    "        DeepMemNet\n",
    "\n",
    "        Param note - changing parameters will require new model file (duh) - this isn't automatic yet\n",
    "        :param vocab_size:\n",
    "        :param story_maxlen:\n",
    "        :param query_maxlen:\n",
    "        :param n_lstm:\n",
    "        :param bidirect:\n",
    "        \"\"\"\n",
    "\n",
    "        # todo: config file for model hyperparams with logging link\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.story_maxlen = story_maxlen\n",
    "        self.query_maxlen = query_maxlen\n",
    "        # placeholders\n",
    "        input_sequence = Input((story_maxlen,), name='StorySeq')\n",
    "        question = Input((query_maxlen,), name='Question')\n",
    "\n",
    "        # Encoders - initial encoders are pretty much just projecting the input into a useful space\n",
    "        # not much need to optimize here really\n",
    "        input_encoder_m = Sequential(name='InputEncoderM')\n",
    "        input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                                      output_dim=64, name='InEncM_Embed'))\n",
    "        input_encoder_m.add(Dropout(0.3))\n",
    "        # output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "        # embed the input into a sequence of vectors of size query_maxlen\n",
    "        input_encoder_c = Sequential(name='InputEncoderC')\n",
    "        input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                                      output_dim=query_maxlen, name='InEncC_Embed'))\n",
    "        \n",
    "        input_encoder_c.add(Dropout(0.3))\n",
    "        \n",
    "        # output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "        # embed the question into a sequence of vectors\n",
    "        question_encoder = Sequential(name='QuestionEncoder')\n",
    "        question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                                       output_dim=64,\n",
    "                                       input_length=query_maxlen, name='QuesEnc_Embed'))\n",
    "        question_encoder.add(Dropout(0.3))\n",
    "        # output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "        # encode input sequence and questions (which are indices)\n",
    "        # to sequences of dense vectors\n",
    "        input_encoded_m = input_encoder_m(input_sequence)\n",
    "        input_encoded_c = input_encoder_c(input_sequence)\n",
    "        question_encoded = question_encoder(question)\n",
    "\n",
    "        # compute a 'match' between the first input vector sequence\n",
    "        # and the question vector sequence\n",
    "        # shape: `(samples, story_maxlen, query_maxlen)`\n",
    "        match = dot([input_encoded_m, question_encoded], axes=(2, 2), name='Match')\n",
    "        match = Activation('softmax', name='MatchActivation')(match)\n",
    "        \n",
    "        match = Conv1D(4, 4, padding='same')(match)\n",
    "\n",
    "        # add the match matrix with the second input vector sequence\n",
    "        response = add([match, input_encoded_c], name='ResponseAdd')  # (samples, story_maxlen, query_maxlen)\n",
    "        response = Permute((2, 1), name='ResponsePermute')(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "        # concatenate the match matrix with the question vector sequence\n",
    "        answer = concatenate([response, question_encoded], name='AnswerConcat')\n",
    "#         answer = Permute((2, 1), name='AnswerPermute')(answer)  # (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "        # Let's try with a time distributed dense before the RNN\n",
    "        if tdd:\n",
    "            answer = TimeDistributed(Dense(n_lstm, name='Answer_TDD'))(answer)\n",
    "            \n",
    "\n",
    "        if bidirect:\n",
    "            # Bidirectional LSTM for better context recognition, plus an additional one for flavor\n",
    "            lstm_rev = Bidirectional(LSTM(n_lstm, return_sequences=True, name='Ans_LSTM_reverse'))\n",
    "            lstm_for = Bidirectional(LSTM(n_lstm, return_sequences=False, name='Ans_LSTM_forward'))\n",
    "        \n",
    "            answer = lstm_rev(answer)  # \"reverse\" pass goes first\n",
    "            answer = lstm_for(answer)\n",
    "        else:\n",
    "            answer = LSTM(n_lstm, name='Answer_LSTM')(answer) # i think this is 'looking' for the context of the answer\n",
    "        # answer = LSTM(n_lstm, name='Ans_LSTM_3)(answer) # Extra LSTM completely runs out of steam at 55% acc! Bidirectional seems to help\n",
    "\n",
    "\n",
    "\n",
    "        # one regularization layer -- more would probably be needed.\n",
    "        answer = Dropout(0.3, name='Answer_Drop')(answer)\n",
    "        answer = Dense(vocab_size, name='Answer_Dense')(answer)  # (samples, vocab_size)\n",
    "        # we output a probability distribution over the vocabulary\n",
    "        answer = Activation('softmax')(answer)\n",
    "\n",
    "        # build the final model\n",
    "        model = Model([input_sequence, question], answer)\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def query(self, storyvec, queryvec):\n",
    "        storyvec = storyvec.reshape((-1, self.story_maxlen))\n",
    "        queryvec = queryvec.reshape((-1, self.query_maxlen))\n",
    "        ans = self.model.predict([storyvec, queryvec])\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dmn = DeepMemNet(bidirect=False, tdd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vis_utils.plot_model(dmn.model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
